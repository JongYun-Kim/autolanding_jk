# @package _global_

# Differential learning rate schedule
# Recommended when encoder, actor, and critic need different learning dynamics
#
# Strategy:
# - Encoder: Slower, stable learning (features should be consistent)
# - Actor: Moderate decay (policy needs to explore then refine)
# - Critic: Faster decay (value estimates should stabilize quickly)
# - Different schedules per network component
#
# Best for: Advanced tuning, addressing specific component learning issues, representation learning

lr_schedule:
  # Encoder: Conservative schedule (features need stability)
  encoder:
    type: step_decay
    intervals:
      - {start: 0, end: 3000000, lr: 8.0e-5}      # Lower initial LR
      - {start: 3000000, end: 10000000, lr: 4.0e-5} # Gentle reduction

  # Actor: Balanced schedule (policy exploration to exploitation)
  actor:
    type: exponential_decay
    intervals:
      - {start: 0, end: 2000000, init_lr: 1.2e-4, decay_rate: 0.999, decay_interval: 5000}
      - {start: 2000000, end: 10000000, init_lr: 6.0e-5, decay_rate: 0.9995, decay_interval: 10000}

  # Critic: Aggressive schedule (value estimates should converge faster)
  critic:
    type: step_decay
    intervals:
      - {start: 0, end: 1000000, lr: 1.5e-4}      # Higher initial LR
      - {start: 1000000, end: 2500000, lr: 7.0e-5}
      - {start: 2500000, end: 10000000, lr: 3.0e-5} # Aggressive reduction
